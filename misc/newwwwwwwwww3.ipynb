{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "\n",
    "# # Check the shape and structure of the extracted data\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "\n",
    "# # Check the shapes of the result\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "# # Expected X shape: (num_samples, 5, 832), y shape: (num_samples, 832)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the LSTM model\n",
    "# model = Sequential([\n",
    "#     LSTM(128, input_shape=(timesteps_in, X.shape[2]), return_sequences=True),\n",
    "#     Dropout(0.2),\n",
    "#     LSTM(128),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(X.shape[2])\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {loss}')\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "\n",
    "# # Check the shape and structure of the extracted data\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "\n",
    "# # Check the shapes of the result\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "# # Expected X shape: (num_samples, 5, 832), y shape: (num_samples, 832)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the LSTM model with L2 regularization\n",
    "# model = Sequential([\n",
    "#     LSTM(128, input_shape=(timesteps_in, X.shape[2]), return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "#     Dropout(0.3),\n",
    "#     LSTM(128, kernel_regularizer=l2(0.01)),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(X.shape[2])\n",
    "# ])\n",
    "\n",
    "# # Compile the model with a lower learning rate\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "# model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32    , validation_data=(X_test, y_test))\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {loss}')\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "\n",
    "# # Check the shape and structure of the extracted data\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "\n",
    "# # Check the shapes of the result\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "# # Expected X shape: (num_samples, 5, 832), y shape: (num_samples, 832)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the LSTM model\n",
    "# model = Sequential([\n",
    "#     LSTM(128, input_shape=(timesteps_in, X.shape[2]), return_sequences=True),\n",
    "#     Dropout(0.2),\n",
    "#     LSTM(128),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(X.shape[2])\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Define early stopping and learning rate scheduler\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "# # Train the model with callbacks\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {loss}')\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "\n",
    "# # Check the shape and structure of the extracted data\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "\n",
    "# # Check the shapes of the result\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "# # Expected X shape: (num_samples, 5, 832), y shape: (num_samples, 832)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the enhanced LSTM model\n",
    "# model = Sequential([\n",
    "#     LSTM(256, return_sequences=True, input_shape=(timesteps_in, X.shape[2])),\n",
    "#     Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     LSTM(256, return_sequences=True),\n",
    "#     Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     LSTM(256),\n",
    "#     Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(X.shape[2])\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Define early stopping and learning rate scheduler\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "# # Train the model with callbacks\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {loss}')\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from tensorflow.keras.optimizers import AdamW\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "\n",
    "# # Check the shape and structure of the extracted data\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "\n",
    "# # Check the shapes of the result\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "# # Expected X shape: (num_samples, 5, 832), y shape: (num_samples, 832)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the enhanced LSTM model\n",
    "# model = Sequential([\n",
    "#     LSTM(256, return_sequences=True, input_shape=(timesteps_in, X.shape[2])),\n",
    "#     Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     LSTM(256, return_sequences=True),\n",
    "#     Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     LSTM(256),\n",
    "#     Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(X.shape[2])\n",
    "# ])\n",
    "\n",
    "# # Use AdamW optimizer\n",
    "# optimizer = AdamW(learning_rate=0.001)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# # Define early stopping and learning rate scheduler\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "# # Train the model with callbacks\n",
    "# model.fit(X_train, y_train, epochs=150, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {loss}')\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "\n",
    "# # Check the shape and structure of the extracted data\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "\n",
    "# # Check the shapes of the result\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "# # Expected X shape: (num_samples, 5, 832), y shape: (num_samples, 832)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the LSTM model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512):\n",
    "#     model = Sequential([\n",
    "#         LSTM(lstm_units, return_sequences=True, input_shape=(timesteps_in, X.shape[2])),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(lstm_units, return_sequences=True),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(lstm_units),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(dense_units, activation='relu'),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(X.shape[2])\n",
    "#     ])\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Custom wrapper to use Keras model with scikit-learn\n",
    "# class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, batch_size=32, epochs=50):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.lstm_units = lstm_units\n",
    "#         self.dense_units = dense_units\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.model_ = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.model_ = create_model(optimizer=self.optimizer, dropout_rate=self.dropout_rate, lstm_units=self.lstm_units, dense_units=self.dense_units)\n",
    "#         self.model_.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, verbose=0)\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.model_.predict(X)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return -mean_squared_error(y, y_pred)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_dist = {\n",
    "#     'optimizer': ['adam', 'rmsprop', AdamW(learning_rate=0.001)],\n",
    "#     'dropout_rate': [0.2, 0.3, 0.4],\n",
    "#     'lstm_units': [128, 256, 512],\n",
    "#     'dense_units': [256, 512, 1024],\n",
    "#     'batch_size': [32, 64, 128],\n",
    "#     'epochs': [50, 100, 150]\n",
    "# }\n",
    "\n",
    "# # Create the KerasRegressor\n",
    "# model = KerasRegressor()\n",
    "\n",
    "# # Set up the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# random_search_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Summarize the results\n",
    "# print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")\n",
    "\n",
    "# # Use the best model to evaluate on the test set\n",
    "# best_model = random_search_result.best_estimator_\n",
    "# test_loss = best_model.score(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# import tensorflow as tf\n",
    "\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the LSTM model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=(timesteps_in, X.shape[2])),\n",
    "#         LSTM(lstm_units, return_sequences=True),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(lstm_units, return_sequences=True),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(lstm_units),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(dense_units, activation='relu'),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(X.shape[2])\n",
    "#     ])\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Custom wrapper to use Keras model with scikit-learn\n",
    "# class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, batch_size=32, epochs=5):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.lstm_units = lstm_units\n",
    "#         self.dense_units = dense_units\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.model_ = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.model_ = create_model(optimizer=self.optimizer, dropout_rate=self.dropout_rate, lstm_units=self.lstm_units, dense_units=self.dense_units)\n",
    "#         self.model_.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, verbose=0)\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.model_.predict(X)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return -mean_squared_error(y, y_pred)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_dist = {\n",
    "#     'optimizer': ['adam', 'rmsprop'],\n",
    "#     'dropout_rate': [0.2, 0.3, 0.4],\n",
    "#     'lstm_units': [128, 256, 512],\n",
    "#     'dense_units': [256, 512, 1024],\n",
    "#     'batch_size': [32, 64, 128],\n",
    "#     'epochs': [50, 100, 150]\n",
    "# }\n",
    "\n",
    "# # Create the KerasRegressor\n",
    "# model = KerasRegressor()\n",
    "\n",
    "# # Set up the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# random_search_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Summarize the results\n",
    "# print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")\n",
    "\n",
    "# # Use the best model to evaluate on the test set\n",
    "# best_model = random_search_result.best_estimator_\n",
    "# test_loss = best_model.score(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# import tensorflow as tf\n",
    "\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the LSTM model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=(timesteps_in, X.shape[2])),\n",
    "#         LSTM(lstm_units, return_sequences=True),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(lstm_units, return_sequences=True),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(lstm_units),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(dense_units, activation='relu'),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(X.shape[2])\n",
    "#     ])\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Custom wrapper to use Keras model with scikit-learn\n",
    "# class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, batch_size=32, epochs=50):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.lstm_units = lstm_units\n",
    "#         self.dense_units = dense_units\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.model_ = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.model_ = create_model(optimizer=self.optimizer, dropout_rate=self.dropout_rate, lstm_units=self.lstm_units, dense_units=self.dense_units)\n",
    "#         self.model_.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, verbose=0)\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.model_.predict(X)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return -mean_squared_error(y, y_pred)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_dist = {\n",
    "#     'optimizer': ['adam', 'rmsprop'],\n",
    "#     'dropout_rate': [0.2, 0.3, 0.4],\n",
    "#     'lstm_units': [128, 256, 512],\n",
    "#     'dense_units': [256, 512, 1024],\n",
    "#     'batch_size': [32, 64, 128],\n",
    "#     'epochs': [50, 100, 150]\n",
    "# }\n",
    "\n",
    "# # Create the KerasRegressor\n",
    "# model = KerasRegressor()\n",
    "\n",
    "# # Set up the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=1, n_jobs=1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# random_search_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Summarize the results\n",
    "# print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")\n",
    "\n",
    "# # Use the best model to evaluate on the test set\n",
    "# best_model = random_search_result.best_estimator_\n",
    "# test_loss = best_model.score(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# import tensorflow as tf\n",
    "\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the LSTM model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=(timesteps_in, X.shape[2])),\n",
    "#         LSTM(lstm_units, return_sequences=True),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(lstm_units, return_sequences=True),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(lstm_units),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(dense_units, activation='relu'),\n",
    "#         Dropout(dropout_rate),\n",
    "#         BatchNormalization(),\n",
    "#         Dense(X.shape[2])\n",
    "#     ])\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Custom wrapper to use Keras model with scikit-learn\n",
    "# class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, batch_size=32, epochs=50):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.lstm_units = lstm_units\n",
    "#         self.dense_units = dense_units\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.model_ = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "#         self.model_ = create_model(optimizer=self.optimizer, dropout_rate=self.dropout_rate, lstm_units=self.lstm_units, dense_units=self.dense_units)\n",
    "#         self.model_.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.model_.predict(X)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return -mean_squared_error(y, y_pred)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_dist = {\n",
    "#     'optimizer': ['adam', 'rmsprop'],\n",
    "#     'dropout_rate': [0.2, 0.3],\n",
    "#     'lstm_units': [128, 256],\n",
    "#     'dense_units': [256, 512],\n",
    "#     'batch_size': [32, 64],\n",
    "#     'epochs': [50, 100]\n",
    "# }\n",
    "\n",
    "# # Use a smaller sample for hyperparameter tuning\n",
    "# X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
    "\n",
    "# # Create the KerasRegressor\n",
    "# model = KerasRegressor()\n",
    "\n",
    "# # Set up the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# random_search_result = random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# # Summarize the results\n",
    "# print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")\n",
    "\n",
    "# # Use the best model to evaluate on the full training set and test set\n",
    "# best_params = random_search_result.best_params_\n",
    "# best_model = KerasRegressor(**best_params)\n",
    "# best_model.fit(X_train, y_train)\n",
    "# test_loss = best_model.score(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "\n",
    "# #best so far\n",
    "# # Mean Squared Error (MSE): 0.0072827399281864495\n",
    "# # Mean Absolute Error (MAE): 0.06142343127173688\n",
    "# # R-squared (R^2): 0.5240704415526208\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Bidirectional, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# import tensorflow as tf\n",
    "\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the advanced LSTM model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, use_bidirectional=False):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps_in, X.shape[2])))\n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(dense_units, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(X.shape[2]))\n",
    "    \n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Custom wrapper to use Keras model with scikit-learn\n",
    "# class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, batch_size=32, epochs=50, use_bidirectional=False):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.lstm_units = lstm_units\n",
    "#         self.dense_units = dense_units\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.use_bidirectional = use_bidirectional\n",
    "#         self.model_ = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "#         reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "#         self.model_ = create_model(optimizer=self.optimizer, dropout_rate=self.dropout_rate, lstm_units=self.lstm_units, dense_units=self.dense_units, use_bidirectional=self.use_bidirectional)\n",
    "#         self.model_.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, verbose=1, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.model_.predict(X)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return -mean_squared_error(y, y_pred)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_dist = {\n",
    "#     'optimizer': ['adam', 'rmsprop'],\n",
    "#     'dropout_rate': [0.2, 0.3],\n",
    "#     'lstm_units': [128, 256],\n",
    "#     'dense_units': [256, 512],\n",
    "#     'batch_size': [32, 64],\n",
    "#     'epochs': [50, 100],\n",
    "#     'use_bidirectional': [True, False]\n",
    "# }\n",
    "\n",
    "# # Use a smaller sample for hyperparameter tuning\n",
    "# X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
    "\n",
    "# # Create the KerasRegressor\n",
    "# model = KerasRegressor()\n",
    "\n",
    "# # Set up the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=1, n_jobs=1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# random_search_result = random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# # Summarize the results\n",
    "# print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")\n",
    "\n",
    "# # Use the best model to evaluate on the full training set and test set\n",
    "# best_params = random_search_result.best_params_\n",
    "# best_model = KerasRegressor(**best_params)\n",
    "# best_model.fit(X_train, y_train)\n",
    "# test_loss = best_model.score(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "# # Mean Squared Error (MSE): 0.006980137275278791\n",
    "# # Mean Absolute Error (MAE): 0.05902950857881014\n",
    "# # R-squared (R^2): 0.5438289683063248\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Bidirectional, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# import tensorflow as tf\n",
    "\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the advanced LSTM model with gradient clipping\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, use_bidirectional=False):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps_in, X.shape[2])))\n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(dense_units, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(X.shape[2]))\n",
    "    \n",
    "#     # Use gradient clipping\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = Adam(clipvalue=1.0)\n",
    "#     elif optimizer == 'rmsprop':\n",
    "#         optimizer = RMSprop(clipvalue=1.0)\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Custom wrapper to use Keras model with scikit-learn\n",
    "# class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, batch_size=32, epochs=50, use_bidirectional=False):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.lstm_units = lstm_units\n",
    "#         self.dense_units = dense_units\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.use_bidirectional = use_bidirectional\n",
    "#         self.model_ = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "#         reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "#         self.model_ = create_model(optimizer=self.optimizer, dropout_rate=self.dropout_rate, lstm_units=self.lstm_units, dense_units=self.dense_units, use_bidirectional=self.use_bidirectional)\n",
    "#         self.model_.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, verbose=1, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.model_.predict(X)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return -mean_squared_error(y, y_pred)\n",
    "\n",
    "# # Define the hyperparameter grid with refined ranges around the best parameters found\n",
    "# param_dist = {\n",
    "#     'optimizer': ['adam', 'rmsprop'],\n",
    "#     'dropout_rate': [0.2, 0.3],\n",
    "#     'lstm_units': [256, 512],\n",
    "#     'dense_units': [512, 1024],\n",
    "#     'batch_size': [32, 64],\n",
    "#     'epochs': [50, 100],\n",
    "#     'use_bidirectional': [True, False]\n",
    "# }\n",
    "\n",
    "# # Use a smaller sample for hyperparameter tuning\n",
    "# X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
    "\n",
    "# # Create the KerasRegressor\n",
    "# model = KerasRegressor()\n",
    "\n",
    "# # Set up the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# random_search_result = random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# # Summarize the results\n",
    "# print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")\n",
    "\n",
    "# # Use the best model to evaluate on the full training set and test set\n",
    "# best_params = random_search_result.best_params_\n",
    "# best_model = KerasRegressor(**best_params)\n",
    "# best_model.fit(X_train, y_train)\n",
    "# test_loss = best_model.score(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "##too forever to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Bidirectional, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# import tensorflow as tf\n",
    "\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the advanced LSTM model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, use_bidirectional=False):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps_in, X.shape[2])))\n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(dense_units, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(X.shape[2]))\n",
    "    \n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Custom wrapper to use Keras model with scikit-learn\n",
    "# class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, batch_size=32, epochs=50, use_bidirectional=False):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.lstm_units = lstm_units\n",
    "#         self.dense_units = dense_units\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.use_bidirectional = use_bidirectional\n",
    "#         self.model_ = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "#         reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "#         self.model_ = create_model(optimizer=self.optimizer, dropout_rate=self.dropout_rate, lstm_units=self.lstm_units, dense_units=self.dense_units, use_bidirectional=self.use_bidirectional)\n",
    "#         self.model_.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, verbose=1, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.model_.predict(X)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return -mean_squared_error(y, y_pred)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_dist = {\n",
    "#     'optimizer': ['adam', 'rmsprop'],\n",
    "#     'dropout_rate': [0.2, 0.3],\n",
    "#     'lstm_units': [128, 256],\n",
    "#     'dense_units': [256, 512],\n",
    "#     'batch_size': [32, 64],\n",
    "#     'epochs': [50, 100],\n",
    "#     'use_bidirectional': [True, False]\n",
    "# }\n",
    "\n",
    "# # Use a smaller sample for hyperparameter tuning\n",
    "# X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
    "\n",
    "# # Create the KerasRegressor\n",
    "# model = KerasRegressor()\n",
    "\n",
    "# # Set up the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# random_search_result = random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# # Summarize the results\n",
    "# print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")\n",
    "\n",
    "# # Use the best model to evaluate on the full training set and test set\n",
    "# best_params = random_search_result.best_params_\n",
    "# best_model = KerasRegressor(**best_params)\n",
    "# best_model.fit(X_train, y_train)\n",
    "# test_loss = best_model.score(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "# #Best: -0.009967062311250588 using {'use_bidirectional': True, 'optimizer': 'adam', 'lstm_units': 256, 'epochs': 100, 'dropout_rate': 0.2, 'dense_units': 256, 'batch_size': 64}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Bidirectional, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# import tensorflow as tf\n",
    "\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the advanced LSTM model with gradient clipping\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, use_bidirectional=False):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps_in, X.shape[2])))\n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(dense_units, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(X.shape[2]))\n",
    "    \n",
    "#     # Use gradient clipping\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = Adam(clipvalue=1.0)\n",
    "#     elif optimizer == 'rmsprop':\n",
    "#         optimizer = RMSprop(clipvalue=1.0)\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Best hyperparameters from RandomizedSearchCV\n",
    "# best_params = {\n",
    "#     'optimizer': 'adam',\n",
    "#     'dropout_rate': 0.2,\n",
    "#     'lstm_units': 256,\n",
    "#     'dense_units': 256,\n",
    "#     'batch_size': 64,\n",
    "#     'epochs': 100,\n",
    "#     'use_bidirectional': True\n",
    "# }\n",
    "\n",
    "# # Create and train the best model\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# model = create_model(\n",
    "#     optimizer=best_params['optimizer'],\n",
    "#     dropout_rate=best_params['dropout_rate'],\n",
    "#     lstm_units=best_params['lstm_units'],\n",
    "#     dense_units=best_params['dense_units'],\n",
    "#     use_bidirectional=best_params['use_bidirectional']\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=best_params['batch_size'],\n",
    "#     epochs=best_params['epochs'],\n",
    "#     validation_split=0.2,\n",
    "#     callbacks=[early_stopping, reduce_lr],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "# # Mean Squared Error (MSE): 0.007433210159811503\n",
    "# # Mean Absolute Error (MAE): 0.062449529734068705\n",
    "# # R-squared (R^2): 0.514242911745018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Bidirectional, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import clear_output\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the advanced LSTM model with gradient clipping\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, use_bidirectional=False):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps_in, X.shape[2])))\n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(dense_units, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(X.shape[2]))\n",
    "    \n",
    "#     # Use gradient clipping\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = Adam(clipvalue=1.0)\n",
    "#     elif optimizer == 'rmsprop':\n",
    "#         optimizer = RMSprop(clipvalue=1.0)\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Best hyperparameters from RandomizedSearchCV\n",
    "# best_params = {\n",
    "#     'optimizer': 'adam',\n",
    "#     'dropout_rate': 0.2,\n",
    "#     'lstm_units': 256,\n",
    "#     'dense_units': 256,\n",
    "#     'batch_size': 64,\n",
    "#     'epochs': 200,  # Increased number of epochs\n",
    "#     'use_bidirectional': True\n",
    "# }\n",
    "\n",
    "# # Create and train the best model\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# class PlotLearning(tf.keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.i = 0metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "# # Mean Squared Error (MSE): 0.007433210159811503\n",
    "# # Mean Absolute Error (MAE): 0.062449529734068705\n",
    "#         self.x = []\n",
    "#         self.losses = []\n",
    "#         self.val_losses = []\n",
    "\n",
    "#         self.fig = plt.figure()\n",
    "        \n",
    "#         self.logs = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         self.logs.append(logs)\n",
    "#         self.x.append(self.i)\n",
    "#         self.losses.append(logs.get('loss'))\n",
    "#         self.val_losses.append(logs.get('val_loss'))\n",
    "#         self.i += 1\n",
    "        \n",
    "#         clear_output(wait=True)\n",
    "#         plt.plot(self.x, self.losses, label=\"loss\")\n",
    "#         plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "# plot_learning = PlotLearning()\n",
    "\n",
    "# model = create_model(\n",
    "#     optimizer=best_params['optimizer'],\n",
    "#     dropout_rate=best_params['dropout_rate'],\n",
    "#     lstm_units=best_params['lstm_units'],\n",
    "#     dense_units=best_params['dense_units'],\n",
    "#     use_bidirectional=best_params['use_bidirectional']\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=best_params['batch_size'],\n",
    "#     epochs=best_params['epochs'],\n",
    "#     validation_split=0.2,\n",
    "#     callbacks=[early_stopping, reduce_lr, plot_learning],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Bidirectional, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import clear_output\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the advanced LSTM model with gradient clipping\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, use_bidirectional=False):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps_in, X.shape[2])))\n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(dense_units, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(X.shape[2]))\n",
    "    \n",
    "#     # Use gradient clipping\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = Adam(clipvalue=1.0)\n",
    "#     elif optimizer == 'rmsprop':\n",
    "#         optimizer = RMSprop(clipvalue=1.0)\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Best hyperparameters from RandomizedSearchCV\n",
    "# best_params = {\n",
    "#     'optimizer': 'adam',\n",
    "#     'dropout_rate': 0.2,\n",
    "#     'lstm_units': 512,\n",
    "#     'dense_units': 512,\n",
    "#     'batch_size': 64,\n",
    "#     'epochs': 50,  # Increased number of epochs\n",
    "#     'use_bidirectional': True\n",
    "# }\n",
    "\n",
    "# # Create and train the best model\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-10)\n",
    "\n",
    "# class PlotLearning(tf.keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.i = 0\n",
    "#         self.x = []\n",
    "#         self.losses = []\n",
    "#         self.val_losses = []\n",
    "\n",
    "#         self.fig = plt.figure()\n",
    "        \n",
    "#         self.logs = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         self.logs.append(logs)\n",
    "#         self.x.append(self.i)\n",
    "#         self.losses.append(logs.get('loss'))\n",
    "#         self.val_losses.append(logs.get('val_loss'))\n",
    "#         self.i += 1\n",
    "        \n",
    "#         clear_output(wait=True)\n",
    "#         plt.plot(self.x, self.losses, label=\"loss\")\n",
    "#         plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "# plot_learning = PlotLearning()\n",
    "\n",
    "# # Function to train and evaluate a single model\n",
    "# def train_and_evaluate_model():\n",
    "#     model = create_model(\n",
    "#         optimizer=best_params['optimizer'],\n",
    "#         dropout_rate=best_params['dropout_rate'],\n",
    "#         lstm_units=best_params['lstm_units'],\n",
    "#         dense_units=best_params['dense_units'],\n",
    "#         use_bidirectional=best_params['use_bidirectional']\n",
    "#     )\n",
    "    \n",
    "#     history = model.fit(\n",
    "#         X_train, y_train,\n",
    "#         batch_size=best_params['batch_size'],\n",
    "#         epochs=best_params['epochs'],\n",
    "#         validation_split=0.2,\n",
    "#         callbacks=[early_stopping, reduce_lr, plot_learning],\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "#     # Evaluate the model on the test set\n",
    "#     test_loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "#     print(f\"Test Loss: {test_loss}\")\n",
    "    \n",
    "#     # Make predictions\n",
    "#     predictions = model.predict(X_test)\n",
    "    \n",
    "#     return predictions\n",
    "\n",
    "# # Train multiple models and average their predictions\n",
    "# n_models = 3\n",
    "# all_predictions = np.zeros((n_models, X_test.shape[0], X_test.shape[2]))\n",
    "\n",
    "# for i in range(n_models):\n",
    "#     print(f\"Training model {i+1}/{n_models}\")\n",
    "#     all_predictions[i] = train_and_evaluate_model()\n",
    "\n",
    "# # Average predictions\n",
    "# ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, ensemble_predictions)\n",
    "# mae = mean_absolute_error(y_test, ensemble_predictions)\n",
    "# r2 = r2_score(y_test, ensemble_predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "# # 50 epochs and 64 batch size\n",
    "# # Mean Squared Error (MSE): 0.010373560889279727\n",
    "# # Mean Absolute Error (MAE): 0.08245649871388501\n",
    "# # R-squared (R^2): 0.32275356126808646\n",
    "\n",
    "\n",
    "# # 50 epochs and 512 batch size \n",
    "# # Mean Squared Error (MSE): 0.010836204595352842\n",
    "# # Mean Absolute Error (MAE): 0.08508739755376513\n",
    "# # R-squared (R^2): 0.2926530175956724\n",
    "\n",
    "# # 50 epochs and 16 batch size\n",
    "# # Mean Squared Error (MSE): 0.009572762265434038\n",
    "# # Mean Absolute Error (MAE): 0.07606218026819941\n",
    "# # R-squared (R^2): 0.374752494537927\n",
    "\n",
    "# # 10 models and 25 epochs and 64 batch size\n",
    "# # Mean Squared Error (MSE): 0.013108292100746278\n",
    "# # Mean Absolute Error (MAE): 0.0952277182630877\n",
    "# # R-squared (R^2): 0.14482415072063837\n",
    "\n",
    "# # Mean Squared Error (MSE): 0.00989403612173151\n",
    "# # Mean Absolute Error (MAE): 0.08034896058249844\n",
    "# # R-squared (R^2): 0.35397119612495054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from scipy.signal import butter, filtfilt\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, BatchNormalization, Bidirectional, concatenate\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import clear_output\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Filtering functions\n",
    "# def butter_lowpass(cutoff, fs, order=5):\n",
    "#     nyq = 0.5 * fs\n",
    "#     normal_cutoff = cutoff / nyq\n",
    "#     b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "#     return b, a\n",
    "\n",
    "# def butter_highpass(cutoff, fs, order=5):\n",
    "#     nyq = 0.5 * fs\n",
    "#     normal_cutoff = cutoff / nyq\n",
    "#     b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "#     return b, a\n",
    "\n",
    "# def lowpass_filter(data, cutoff, fs, order=5):\n",
    "#     b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "#     padlen = min(10, len(data[0]) - 1)\n",
    "#     y = np.apply_along_axis(lambda m: filtfilt(b, a, m, padlen=padlen), axis=1, arr=data)\n",
    "#     return y\n",
    "\n",
    "# def highpass_filter(data, cutoff, fs, order=5):\n",
    "#     b, a = butter_highpass(cutoff, fs, order=order)\n",
    "#     padlen = min(10, len(data[0]) - 1)\n",
    "#     y = np.apply_along_axis(lambda m: filtfilt(b, a, m, padlen=padlen), axis=1, arr=data)\n",
    "#     return y\n",
    "\n",
    "# # Define cutoff frequencies and sampling frequency\n",
    "# cutoff_low = 0.1\n",
    "# cutoff_high = 0.1\n",
    "# fs = 1.0\n",
    "\n",
    "# # Apply filters to the features\n",
    "# X_train_low = lowpass_filter(X_train, cutoff_low, fs)\n",
    "# X_train_high = highpass_filter(X_train, cutoff_high, fs)\n",
    "# X_test_low = lowpass_filter(X_test, cutoff_low, fs)\n",
    "# X_test_high = highpass_filter(X_test, cutoff_high, fs)\n",
    "\n",
    "# print(f'X_train_low shape: {X_train_low.shape}, X_train_high shape: {X_train_high.shape}')\n",
    "# print(f'X_test_low shape: {X_test_low.shape}, X_test_high shape: {X_test_high.shape}')\n",
    "\n",
    "# # Define a function to create the multi-input LSTM model with gradient clipping\n",
    "# def create_multi_input_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, use_bidirectional=False):\n",
    "#     input_low = Input(shape=(timesteps_in, X_train_low.shape[2]))\n",
    "#     input_high = Input(shape=(timesteps_in, X_train_high.shape[2]))\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         x_low = Bidirectional(LSTM(lstm_units, return_sequences=True))(input_low)\n",
    "#         x_high = Bidirectional(LSTM(lstm_units, return_sequences=True))(input_high)\n",
    "#     else:\n",
    "#         x_low = LSTM(lstm_units, return_sequences=True)(input_low)\n",
    "#         x_high = LSTM(lstm_units, return_sequences=True)(input_high)\n",
    "    \n",
    "#     x_low = Dropout(dropout_rate)(x_low)\n",
    "#     x_low = BatchNormalization()(x_low)\n",
    "#     x_high = Dropout(dropout_rate)(x_high)\n",
    "#     x_high = BatchNormalization()(x_high)\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         x_low = Bidirectional(LSTM(lstm_units, return_sequences=True))(x_low)\n",
    "#         x_high = Bidirectional(LSTM(lstm_units, return_sequences=True))(x_high)\n",
    "#     else:\n",
    "#         x_low = LSTM(lstm_units, return_sequences=True)(x_low)\n",
    "#         x_high = LSTM(lstm_units, return_sequences=True)(x_high)\n",
    "    \n",
    "#     x_low = Dropout(dropout_rate)(x_low)\n",
    "#     x_low = BatchNormalization()(x_low)\n",
    "#     x_high = Dropout(dropout_rate)(x_high)\n",
    "#     x_high = BatchNormalization()(x_high)\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         x_low = Bidirectional(LSTM(lstm_units))(x_low)\n",
    "#         x_high = Bidirectional(LSTM(lstm_units))(x_high)\n",
    "#     else:\n",
    "#         x_low = LSTM(lstm_units)(x_low)\n",
    "#         x_high = LSTM(lstm_units)(x_high)\n",
    "    \n",
    "#     x_low = Dropout(dropout_rate)(x_low)\n",
    "#     x_low = BatchNormalization()(x_low)\n",
    "#     x_high = Dropout(dropout_rate)(x_high)\n",
    "#     x_high = BatchNormalization()(x_high)\n",
    "    \n",
    "#     merged = concatenate([x_low, x_high])\n",
    "    \n",
    "#     x = Dense(dense_units, activation='relu')(merged)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     output = Dense(X_train.shape[2])(x)\n",
    "    \n",
    "#     # Use gradient clipping\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = Adam(clipvalue=1.0)\n",
    "#     elif optimizer == 'rmsprop':\n",
    "#         optimizer = RMSprop(clipvalue=1.0)\n",
    "\n",
    "#     model = Model(inputs=[input_low, input_high], outputs=output)\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Best hyperparameters from RandomizedSearchCV\n",
    "# best_params = {\n",
    "#     'optimizer': 'adam',\n",
    "#     'dropout_rate': 0.2,\n",
    "#     'lstm_units': 256,\n",
    "#     'dense_units': 256,\n",
    "#     'batch_size': 256,  # Increased batch size\n",
    "#     'epochs': 200,  # Increased number of epochs\n",
    "#     'use_bidirectional': True\n",
    "# }\n",
    "\n",
    "# # Create and train the best model\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# class PlotLearning(tf.keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.i = 0\n",
    "#         self.x = []\n",
    "#         self.losses = []\n",
    "#         self.val_losses = []\n",
    "\n",
    "#         self.fig = plt.figure()\n",
    "        \n",
    "#         self.logs = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         self.logs.append(logs)\n",
    "#         self.x.append(self.i)\n",
    "#         self.losses.append(logs.get('loss'))\n",
    "#         self.val_losses.append(logs.get('val_loss'))\n",
    "#         self.i += 1\n",
    "        \n",
    "#         clear_output(wait=True)\n",
    "#         plt.plot(self.x, self.losses, label=\"loss\")\n",
    "#         plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "# plot_learning = PlotLearning()\n",
    "\n",
    "# # Train the multi-input model\n",
    "# model = create_multi_input_model(\n",
    "#     optimizer=best_params['optimizer'],\n",
    "#     dropout_rate=best_params['dropout_rate'],\n",
    "#     lstm_units=best_params['lstm_units'],\n",
    "#     dense_units=best_params['dense_units'],\n",
    "#     use_bidirectional=best_params['use_bidirectional']\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train_low, X_train_high], y_train,\n",
    "#     batch_size=best_params['batch_size'],\n",
    "#     epochs=best_params['epochs'],\n",
    "#     validation_split=0.2,\n",
    "#     callbacks=[early_stopping, reduce_lr, plot_learning],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss = model.evaluate([X_test_low, X_test_high], y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict([X_test_low, X_test_high])\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Bidirectional, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import clear_output\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the advanced LSTM model with gradient clipping\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, lstm_units=256, dense_units=512, use_bidirectional=False):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps_in, X.shape[2])))\n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if use_bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(lstm_units)))\n",
    "#     else:\n",
    "#         model.add(LSTM(lstm_units))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(dense_units, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(X.shape[2]))\n",
    "    \n",
    "#     # Use gradient clipping\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = Adam(clipvalue=1.0)\n",
    "#     elif optimizer == 'rmsprop':\n",
    "#         optimizer = RMSprop(clipvalue=1.0)\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Best hyperparameters from RandomizedSearchCV\n",
    "# best_params = {\n",
    "#     'optimizer': 'adam',\n",
    "#     'dropout_rate': 0.2,\n",
    "#     'lstm_units': 512,\n",
    "#     'dense_units': 512,\n",
    "#     'batch_size': 64,\n",
    "#     'epochs': 50,  # Increased number of epochs\n",
    "#     'use_bidirectional': True\n",
    "# }\n",
    "\n",
    "# # Create and train the best model\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-10)\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# def lr_scheduler(epoch, lr):\n",
    "#     if epoch > 10:\n",
    "#         lr = lr * tf.math.exp(-0.1)\n",
    "#     return lr\n",
    "\n",
    "# lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# class PlotLearning(tf.keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.i = 0\n",
    "#         self.x = []\n",
    "#         self.losses = []\n",
    "#         self.val_losses = []\n",
    "\n",
    "#         self.fig = plt.figure()\n",
    "        \n",
    "#         self.logs = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         self.logs.append(logs)\n",
    "#         self.x.append(self.i)\n",
    "#         self.losses.append(logs.get('loss'))\n",
    "#         self.val_losses.append(logs.get('val_loss'))\n",
    "#         self.i += 1\n",
    "        \n",
    "#         clear_output(wait=True)\n",
    "#         plt.plot(self.x, self.losses, label=\"loss\")\n",
    "#         plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "# plot_learning = PlotLearning()\n",
    "\n",
    "# # Function to train and evaluate a single model\n",
    "# def train_and_evaluate_model():\n",
    "#     model = create_model(\n",
    "#         optimizer=best_params['optimizer'],\n",
    "#         dropout_rate=best_params['dropout_rate'],\n",
    "#         lstm_units=best_params['lstm_units'],\n",
    "#         dense_units=best_params['dense_units'],\n",
    "#         use_bidirectional=best_params['use_bidirectional']\n",
    "#     )\n",
    "    \n",
    "#     history = model.fit(\n",
    "#         X_train, y_train,\n",
    "#         batch_size=best_params['batch_size'],\n",
    "#         epochs=best_params['epochs'],\n",
    "#         validation_split=0.2,\n",
    "#         callbacks=[early_stopping, reduce_lr, lr_callback, plot_learning],\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "#     # Evaluate the model on the test set\n",
    "#     test_loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "#     print(f\"Test Loss: {test_loss}\")\n",
    "    \n",
    "#     # Make predictions\n",
    "#     predictions = model.predict(X_test)\n",
    "    \n",
    "#     return predictions\n",
    "\n",
    "# # Train multiple models and average their predictions\n",
    "# n_models = 3\n",
    "# all_predictions = np.zeros((n_models, X_test.shape[0], X_test.shape[2]))\n",
    "\n",
    "# for i in range(n_models):\n",
    "#     print(f\"Training model {i+1}/{n_models}\")\n",
    "#     all_predictions[i] = train_and_evaluate_model()\n",
    "\n",
    "# # Average predictions\n",
    "# ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, ensemble_predictions)\n",
    "# mae = mean_absolute_error(y_test, ensemble_predictions)\n",
    "# r2 = r2_score(y_test, ensemble_predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 23:20:25.746859: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.16.1\n",
      "(50, 1000, 832)\n",
      "X shape: (49750, 5, 832), y shape: (49750, 832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 23:20:35.650270: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:20:35.671860: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:20:35.674565: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:20:35.679346: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:20:35.682566: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:20:35.686118: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "cudaSetDevice() on GPU:0 failed. Status: CUDA-capable device(s) is/are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m plot_learning \u001b[38;5;241m=\u001b[39m PlotLearning()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Train the TCN model\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m model \u001b[38;5;241m=\u001b[39m create_tcn_model(\n\u001b[1;32m    117\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    118\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    119\u001b[0m     filters\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    120\u001b[0m     kernel_size\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    121\u001b[0m     dense_units\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense_units\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    124\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    125\u001b[0m     X_train, y_train,\n\u001b[1;32m    126\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m, in \u001b[0;36mcreate_tcn_model\u001b[0;34m(optimizer, dropout_rate, filters, kernel_size, dense_units)\u001b[0m\n\u001b[1;32m     45\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Input(shape\u001b[38;5;241m=\u001b[39m(timesteps_in, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])))\n\u001b[0;32m---> 48\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv1D(filters, kernel_size, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(dropout_rate))\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39madd(BatchNormalization())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/models/sequential.py:120\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer, rebuild)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_rebuild()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/models/sequential.py:139\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    138\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(input_shape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:223\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(original_build_method)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[0;32m--> 223\u001b[0m         original_build_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/models/sequential.py:183\u001b[0m, in \u001b[0;36mSequential.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:605\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetRunEagerOpAsFunction(opts, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    603\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetJitCompileRewrite(\n\u001b[1;32m    604\u001b[0m       opts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile_rewrite)\n\u001b[0;32m--> 605\u001b[0m   context_handle \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_NewContext(opts)\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_DeleteContextOptions(opts)\n",
      "\u001b[0;31mInternalError\u001b[0m: cudaSetDevice() on GPU:0 failed. Status: CUDA-capable device(s) is/are busy or unavailable"
     ]
    }
   ],
   "source": [
    "# import scipy.io\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, BatchNormalization, Input\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import clear_output\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Verify TensorFlow installation\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# # Load the .mat file\n",
    "# file_path = './EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "# data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# # Extract the relevant data\n",
    "# EV_data = data['EV_re_im_split']\n",
    "# data = EV_data\n",
    "# del EV_data\n",
    "# print(data.shape)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, timesteps_in):\n",
    "#     X, y = [], []\n",
    "#     for ue in data:\n",
    "#         for i in range(len(ue) - timesteps_in):\n",
    "#             X.append(ue[i:i + timesteps_in])\n",
    "#             y.append(ue[i + timesteps_in])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# timesteps_in = 5\n",
    "\n",
    "# X, y = create_sequences(data, timesteps_in)\n",
    "# print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a function to create the TCN model\n",
    "# def create_tcn_model(optimizer='adam', dropout_rate=0.3, filters=64, kernel_size=3, dense_units=512):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps_in, X.shape[2])))\n",
    "    \n",
    "#     model.add(Conv1D(filters, kernel_size, padding='causal', activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv1D(filters, kernel_size, padding='causal', activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv1D(filters, kernel_size, padding='causal', activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(dense_units, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(X.shape[2]))\n",
    "\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = Adam(clipvalue=1.0)\n",
    "#     elif optimizer == 'rmsprop':\n",
    "#         optimizer = RMSprop(clipvalue=1.0)\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Best hyperparameters from RandomizedSearchCV\n",
    "# best_params = {\n",
    "#     'optimizer': 'adam',\n",
    "#     'dropout_rate': 0.2,\n",
    "#     'filters': 64,\n",
    "#     'kernel_size': 3,\n",
    "#     'dense_units': 512,\n",
    "#     'batch_size': 16,\n",
    "#     'epochs': 200  # You can adjust this based on your needs\n",
    "# }\n",
    "\n",
    "# # Create and train the best model\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-8)\n",
    "\n",
    "# class PlotLearning(tf.keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.i = 0\n",
    "#         self.x = []\n",
    "#         self.losses = []\n",
    "#         self.val_losses = []\n",
    "\n",
    "#         self.fig = plt.figure()\n",
    "        \n",
    "#         self.logs = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         self.logs.append(logs)\n",
    "#         self.x.append(self.i)\n",
    "#         self.losses.append(logs.get('loss'))\n",
    "#         self.val_losses.append(logs.get('val_loss'))\n",
    "#         self.i += 1\n",
    "        \n",
    "#         clear_output(wait=True)\n",
    "#         plt.plot(self.x, self.losses, label=\"loss\")\n",
    "#         plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "# plot_learning = PlotLearning()\n",
    "\n",
    "# # Train the TCN model\n",
    "# model = create_tcn_model(\n",
    "#     optimizer=best_params['optimizer'],\n",
    "#     dropout_rate=best_params['dropout_rate'],\n",
    "#     filters=best_params['filters'],\n",
    "#     kernel_size=best_params['kernel_size'],\n",
    "#     dense_units=best_params['dense_units']\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=best_params['batch_size'],\n",
    "#     epochs=best_params['epochs'],\n",
    "#     validation_split=0.2,\n",
    "#     callbacks=[early_stopping, reduce_lr, plot_learning],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# print(f'Mean Squared Error (MSE): {mse}')\n",
    "# print(f'Mean Absolute Error (MAE): {mae}')\n",
    "# print(f'R-squared (R^2): {r2}')\n",
    "\n",
    "\n",
    "# # Mean Squared Error (MSE): 0.009564422473049478\n",
    "# # Mean Absolute Error (MAE): 0.07355094764741145\n",
    "# # R-squared (R^2): 0.37527507499472795\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
