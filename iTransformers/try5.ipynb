{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1000, 832)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from iTransformer import iTransformer\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/rithvik/iitm2/csi-prediction/EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "data = scipy.io.loadmat(file_path)\n",
    "data = data['EV_re_im_split']\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rithvik/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model = iTransformer(\n",
    "    num_variates = 832,\n",
    "    lookback_len = 1000,                  # or the lookback length in the paper\n",
    "    dim = 256,                          # model dimensions\n",
    "    depth = 6,                          # depth\n",
    "    heads = 8,                          # attention heads\n",
    "    dim_head = 64,                      # head dimension\n",
    "    pred_length = (5,),     # can be one prediction, or many\n",
    "    num_tokens_per_variate = 1,         # experimental setting that projects each variate to more than one token. the idea is that the network can learn to divide up into time tokens for more granular attention across time. thanks to flash attention, you should be able to accommodate long sequence lengths just fine\n",
    "    use_reversible_instance_norm = True # use reversible instance normalization, proposed here https://openreview.net/forum?id=cGDAkQo1C0p . may be redundant given the layernorms within iTransformer (and whatever else attention learns emergently on the first layer, prior to the first layernorm). if i come across some time, i'll gather up all the statistics across variates, project them, and condition the transformer a bit further. that makes more sense\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1187, -0.2174,  0.0034,  ...,  0.0242,  0.0633,  0.0953],\n",
       "         [-0.0384,  0.0744, -0.0009,  ...,  0.1139,  0.0410,  0.1060],\n",
       "         [ 0.0382, -0.0018,  0.0033,  ..., -0.0694,  0.0537,  0.1202],\n",
       "         [ 0.0118, -0.0390, -0.0491,  ...,  0.1146, -0.0253, -0.0529],\n",
       "         [ 0.0871,  0.0953,  0.0957,  ..., -0.0151, -0.0175, -0.0072]],\n",
       "\n",
       "        [[-0.0682,  0.1956,  0.1479,  ...,  0.0208,  0.1254,  0.0691],\n",
       "         [ 0.2183, -0.0943,  0.0959,  ...,  0.0056, -0.0369,  0.1261],\n",
       "         [ 0.0362, -0.0638,  0.0322,  ...,  0.0769, -0.0202, -0.0428],\n",
       "         [ 0.0874, -0.1144,  0.0632,  ...,  0.0152,  0.1154, -0.0619],\n",
       "         [-0.0302,  0.0196, -0.0317,  ..., -0.0653,  0.0592, -0.0477]],\n",
       "\n",
       "        [[-0.0775,  0.0430, -0.0264,  ...,  0.1098,  0.1095,  0.0565],\n",
       "         [ 0.1970,  0.0269,  0.1882,  ...,  0.1035, -0.1654,  0.0505],\n",
       "         [-0.1152,  0.0156, -0.1032,  ...,  0.0321,  0.0004,  0.0174],\n",
       "         [-0.0007, -0.0139, -0.0999,  ...,  0.0723,  0.1736, -0.0240],\n",
       "         [ 0.0061,  0.0311, -0.1826,  ...,  0.0109, -0.1707,  0.0081]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0543,  0.0953,  0.0438,  ..., -0.0471,  0.0294, -0.0618],\n",
       "         [ 0.1511,  0.0976,  0.0772,  ...,  0.1737,  0.1562,  0.2049],\n",
       "         [-0.0876,  0.0853, -0.0333,  ...,  0.0387, -0.0051, -0.0244],\n",
       "         [ 0.0670,  0.0313, -0.0619,  ...,  0.0100,  0.0570, -0.0255],\n",
       "         [ 0.0419, -0.0099, -0.0387,  ..., -0.0161, -0.1191, -0.0518]],\n",
       "\n",
       "        [[-0.0485, -0.0447, -0.0358,  ...,  0.1583,  0.1532,  0.1445],\n",
       "         [ 0.1485,  0.1422,  0.1855,  ...,  0.1158,  0.0671,  0.0726],\n",
       "         [-0.0377, -0.0309, -0.0239,  ...,  0.0463,  0.0699,  0.0707],\n",
       "         [-0.0212, -0.0473,  0.0202,  ..., -0.0830, -0.1134, -0.1133],\n",
       "         [ 0.0826,  0.0861,  0.0651,  ...,  0.0401, -0.0122, -0.0097]],\n",
       "\n",
       "        [[ 0.0825,  0.1056,  0.1153,  ...,  0.0102, -0.0611,  0.1809],\n",
       "         [ 0.0243,  0.0540, -0.0790,  ...,  0.2326,  0.1026, -0.0179],\n",
       "         [ 0.0223, -0.0694,  0.0324,  ..., -0.0300, -0.0504,  0.0535],\n",
       "         [-0.0260, -0.0644,  0.0500,  ...,  0.0419,  0.0674, -0.0306],\n",
       "         [-0.0724,  0.0390, -0.1406,  ...,  0.0257, -0.0077, -0.0829]]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0034, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[5][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=preds[5]\n",
    "del preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 5, 832])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
