{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "file_path = '../EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "data = scipy.io.loadmat(file_path)\n",
    "data = data['EV_re_im_split']\n",
    "\n",
    "feature_len=832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape(data.shape[0], -1)  # shape becomes (2100, 398*256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_UEs, num_timesteps = data.shape\n",
    "sequence_length = 10  # Use the first 10 values as input\n",
    "stride = 1  # Moving one step at a time\n",
    "\n",
    "# Initialize lists to hold input and output sequences\n",
    "X, y = [], []\n",
    "\n",
    "# Loop through each UE\n",
    "for ue_idx in range(num_UEs):\n",
    "    ue_data = data[ue_idx]\n",
    "    num_sequences = (num_timesteps - sequence_length) // stride\n",
    "\n",
    "    # Extract input sequences\n",
    "    input_indices = np.arange(sequence_length) + np.arange(num_sequences)[:, None]\n",
    "    input_seq = ue_data[input_indices]\n",
    "\n",
    "    # Extract output values\n",
    "    output_indices = np.arange(sequence_length, num_sequences + sequence_length)\n",
    "    output_value = ue_data[output_indices]\n",
    "\n",
    "    # Append to the lists\n",
    "    X.append(input_seq)\n",
    "    y.append(output_value)\n",
    "\n",
    "# Convert lists to NumPy arrays for easier processing\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train-test split ratio (e.g., 80% train, 20% test)\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(X) * train_ratio)\n",
    "\n",
    "# Split the data into training and testing while keeping the time series intact\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Check the resulting shapes\n",
    "print(f\"Training data shapes: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Testing data shapes: X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, ReLU, Dropout, Add, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Simplified TCN residual block\n",
    "def residual_block(x, dilation_rate, num_filters, kernel_size, dropout_rate=0.0):\n",
    "    conv1 = Conv1D(num_filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    bn1 = BatchNormalization()(conv1)\n",
    "    act1 = ReLU()(bn1)\n",
    "    drop1 = Dropout(dropout_rate)(act1)\n",
    "\n",
    "    # Removed second convolutional layer and its components\n",
    "\n",
    "    if x.shape[-1] != num_filters:\n",
    "        x = Conv1D(num_filters, kernel_size=1)(x)\n",
    "\n",
    "    out = Add()([x, drop1])\n",
    "    out = ReLU()(out)\n",
    "    return out\n",
    "\n",
    "# Build simplified TCN model\n",
    "def build_tcn_model(input_shape, num_blocks=2, num_filters=32, kernel_size=3, dropout_rate=0.2):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        dilation_rate = 2 ** i\n",
    "        x = residual_block(x, dilation_rate, num_filters, kernel_size, dropout_rate)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_length = 10  # The number of previous time steps used for input\n",
    "num_features = 1  # Modify according to your dataset\n",
    "\n",
    "# Adjust `num_features` if your input sequences contain multiple features.\n",
    "model = build_tcn_model(input_shape=(input_length, num_features))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.save('best_tcn_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Assuming you have `X_train`, `y_train`, `X_test`, and `y_test` arrays loaded from preprocessing\n",
    "# Make sure the input data is reshaped to have the expected dimensions (num_samples, sequence_length, num_features)\n",
    "\n",
    "# Reshape input to have the required dimensions: (samples, input_length, num_features)\n",
    "num_features = 1  # Adjust based on the dataset (e.g., 1 for univariate, multiple if needed)\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], num_features))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], num_features))\n",
    "\n",
    "# Initialize the TCN model\n",
    "model = build_tcn_model(input_shape=(X_train.shape[1], num_features))\n",
    "\n",
    "# Callbacks for training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_tcn_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_reshaped, y_train,\n",
    "    validation_data=(X_test_reshaped, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training history for loss/accuracy improvement\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss History')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
