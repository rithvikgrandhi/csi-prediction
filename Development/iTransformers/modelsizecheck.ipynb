{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from iTransformer import iTransformer2D\n",
    "    \n",
    "# Set up parameters and data\n",
    "num_UEs=50\n",
    "num_timesteps=1000 \n",
    "num_features =832\n",
    "lookback_len = 10\n",
    "forecast_len = 5\n",
    "\n",
    "\n",
    "# Define the iTransformer model\n",
    "model = iTransformer2D(\n",
    "    num_variates=num_features,\n",
    "    lookback_len=lookback_len,\n",
    "    dim=832,           # Model dimension\n",
    "    depth=10,           # Number of layers\n",
    "    heads=16,           # Number of attention heads\n",
    "    dim_head=128,       # Dimension per head\n",
    "    pred_length=forecast_len,  # Prediction horizon (1 in this case)\n",
    "    num_time_tokens=2,  # Single token per variate\n",
    "    use_reversible_instance_norm=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iTransformer2D(\n",
      "  (reversible_instance_norm): RevIN()\n",
      "  (layers): ModuleList(\n",
      "    (0-9): 10 x ModuleList(\n",
      "      (0): SimpleGateLoopLayer(\n",
      "        (norm): RMSNorm()\n",
      "        (to_qkva): Sequential(\n",
      "          (0): Linear(in_features=832, out_features=2496, bias=False)\n",
      "          (1): Rearrange('b n (qkva d) -> qkva (b d) n 1', qkva=3)\n",
      "        )\n",
      "        (maybe_post_ln): Identity()\n",
      "        (split_heads): Rearrange('(b d) n 1 -> b n d', d=832)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (attn): Attention(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (to_qkv): Sequential(\n",
      "            (0): Linear(in_features=832, out_features=6144, bias=False)\n",
      "            (1): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=16)\n",
      "          )\n",
      "          (to_v_gates): Sequential(\n",
      "            (0): Linear(in_features=832, out_features=2048, bias=False)\n",
      "            (1): SiLU()\n",
      "            (2): Rearrange('b n (h d) -> b h n d', h=16)\n",
      "          )\n",
      "          (attend): Attend(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (to_out): Sequential(\n",
      "            (0): Rearrange('b h n d -> b n (h d)')\n",
      "            (1): Linear(in_features=2048, out_features=832, bias=False)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=832, out_features=4436, bias=True)\n",
      "          (1): GEGLU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=2218, out_features=832, bias=True)\n",
      "        )\n",
      "        (attn_norm): LayerNorm((832,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_norm): LayerNorm((832,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attn): Attention(\n",
      "          (to_qkv): Sequential(\n",
      "            (0): Linear(in_features=832, out_features=6144, bias=False)\n",
      "            (1): Rearrange('b n (qkv h d) -> qkv b h n d', qkv=3, h=16)\n",
      "          )\n",
      "          (to_v_gates): Sequential(\n",
      "            (0): Linear(in_features=832, out_features=2048, bias=False)\n",
      "            (1): SiLU()\n",
      "            (2): Rearrange('b n (h d) -> b h n d', h=16)\n",
      "          )\n",
      "          (attend): Attend(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (to_out): Sequential(\n",
      "            (0): Rearrange('b h n d -> b n (h d)')\n",
      "            (1): Linear(in_features=2048, out_features=832, bias=False)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=832, out_features=4436, bias=True)\n",
      "          (1): GEGLU()\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=2218, out_features=832, bias=True)\n",
      "        )\n",
      "        (attn_norm): LayerNorm((832,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_norm): LayerNorm((832,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_variate_token): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=832, bias=True)\n",
      "    (1): LayerNorm((832,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (to_time_tokens): Sequential(\n",
      "    (0): Rearrange('b v n -> (b v) 1 n')\n",
      "    (1): ConstantPad1d(padding=(5, 0), value=0.0)\n",
      "    (2): Conv1d(1, 832, kernel_size=(10,), stride=(1,))\n",
      "    (3): Rearrange('(b v) d t -> b v t d', v=832)\n",
      "    (4): LayerNorm((832,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pred_heads): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=832, out_features=5, bias=True)\n",
      "      (1): Rearrange('b v n -> b n v')\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 167.12 MiB is free. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 2.52 GiB is allocated by PyTorch, and 332.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m----> 2\u001b[0m model_summary \u001b[38;5;241m=\u001b[39m summary(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m832\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_summary)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchinfo/torchinfo.py:220\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[0;32m--> 220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[1;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchinfo/torchinfo.py:256\u001b[0m, in \u001b[0;36mprocess_input\u001b[0;34m(input_data, input_size, batch_dim, device, dtypes)\u001b[0m\n\u001b[1;32m    254\u001b[0m         dtypes \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfloat] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_size)\n\u001b[1;32m    255\u001b[0m     correct_input_size \u001b[38;5;241m=\u001b[39m get_correct_input_sizes(input_size)\n\u001b[0;32m--> 256\u001b[0m     x \u001b[38;5;241m=\u001b[39m get_input_tensor(correct_input_size, batch_dim, dtypes, device)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, correct_input_size\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchinfo/torchinfo.py:532\u001b[0m, in \u001b[0;36mget_input_tensor\u001b[0;34m(input_size, batch_dim, dtypes, device)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39mbatch_dim)\n\u001b[0;32m--> 532\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(input_tensor\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mtype(dtype))\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 167.12 MiB is free. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 2.52 GiB is allocated by PyTorch, and 332.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model_summary = summary(model, input_size=(50, 1000, 832))\n",
    "print(model_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
