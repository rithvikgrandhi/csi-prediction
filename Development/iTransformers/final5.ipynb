{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1000, 832)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from iTransformer import iTransformer\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/rithvik/iitm2/csi-prediction/EV_Rank_1_52_RBs_50_UEs_1000_snaps.mat'\n",
    "data = scipy.io.loadmat(file_path)\n",
    "data = data['EV_re_im_split']\n",
    "print(data.shape)\n",
    "\n",
    "data2=data[:,:-5,:]\n",
    "data3=data[:,:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 995, 832), (50, 5, 832))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape, data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "model = iTransformer(\n",
    "    num_variates = 832,\n",
    "    lookback_len = 995,                  # or the lookback length in the paper\n",
    "    dim = 256,                          # model dimensions\n",
    "    depth = 6,                          # depth\n",
    "    heads = 8,                          # attention heads\n",
    "    dim_head = 64,                      # head dimension\n",
    "    pred_length = (5,),     # can be one prediction, or many\n",
    "    num_tokens_per_variate = 1,         # experimental setting that projects each variate to more than one token. the idea is that the network can learn to divide up into time tokens for more granular attention across time. thanks to flash attention, you should be able to accommodate long sequence lengths just fine\n",
    "    use_reversible_instance_norm = True # use reversible instance normalization, proposed here https://openreview.net/forum?id=cGDAkQo1C0p . may be redundant given the layernorms within iTransformer (and whatever else attention learns emergently on the first layer, prior to the first layernorm). if i come across some time, i'll gather up all the statistics across variates, project them, and condition the transformer a bit further. that makes more sense\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "iTransformer                                  [50, 5, 832]              1,024\n",
       "├─RevIN: 1-1                                  [50, 832, 995]            (1,664)\n",
       "├─Sequential: 1-2                             [50, 832, 256]            --\n",
       "│    └─Linear: 2-1                            [50, 832, 256]            254,976\n",
       "│    └─Rearrange: 2-2                         [50, 832, 256]            --\n",
       "│    └─LayerNorm: 2-3                         [50, 832, 256]            512\n",
       "├─ModuleList: 1-3                             --                        --\n",
       "│    └─ModuleList: 2-4                        --                        --\n",
       "│    │    └─Attention: 3-1                    [50, 836, 256]            655,360\n",
       "│    │    └─LayerNorm: 3-2                    [50, 836, 256]            512\n",
       "│    │    └─Sequential: 3-3                   [50, 836, 256]            525,396\n",
       "│    │    └─LayerNorm: 3-4                    [50, 836, 256]            512\n",
       "│    └─ModuleList: 2-5                        --                        --\n",
       "│    │    └─Attention: 3-5                    [50, 836, 256]            655,360\n",
       "│    │    └─LayerNorm: 3-6                    [50, 836, 256]            512\n",
       "│    │    └─Sequential: 3-7                   [50, 836, 256]            525,396\n",
       "│    │    └─LayerNorm: 3-8                    [50, 836, 256]            512\n",
       "│    └─ModuleList: 2-6                        --                        --\n",
       "│    │    └─Attention: 3-9                    [50, 836, 256]            655,360\n",
       "│    │    └─LayerNorm: 3-10                   [50, 836, 256]            512\n",
       "│    │    └─Sequential: 3-11                  [50, 836, 256]            525,396\n",
       "│    │    └─LayerNorm: 3-12                   [50, 836, 256]            512\n",
       "│    └─ModuleList: 2-7                        --                        --\n",
       "│    │    └─Attention: 3-13                   [50, 836, 256]            655,360\n",
       "│    │    └─LayerNorm: 3-14                   [50, 836, 256]            512\n",
       "│    │    └─Sequential: 3-15                  [50, 836, 256]            525,396\n",
       "│    │    └─LayerNorm: 3-16                   [50, 836, 256]            512\n",
       "│    └─ModuleList: 2-8                        --                        --\n",
       "│    │    └─Attention: 3-17                   [50, 836, 256]            655,360\n",
       "│    │    └─LayerNorm: 3-18                   [50, 836, 256]            512\n",
       "│    │    └─Sequential: 3-19                  [50, 836, 256]            525,396\n",
       "│    │    └─LayerNorm: 3-20                   [50, 836, 256]            512\n",
       "│    └─ModuleList: 2-9                        --                        --\n",
       "│    │    └─Attention: 3-21                   [50, 836, 256]            655,360\n",
       "│    │    └─LayerNorm: 3-22                   [50, 836, 256]            512\n",
       "│    │    └─Sequential: 3-23                  [50, 836, 256]            525,396\n",
       "│    │    └─LayerNorm: 3-24                   [50, 836, 256]            512\n",
       "├─ModuleList: 1-4                             --                        --\n",
       "│    └─Sequential: 2-10                       [50, 5, 832]              --\n",
       "│    │    └─Rearrange: 3-25                   [50, 832, 256]            --\n",
       "│    │    └─Linear: 3-26                      [50, 832, 5]              1,285\n",
       "│    │    └─Rearrange: 3-27                   [50, 5, 832]              --\n",
       "===============================================================================================\n",
       "Total params: 7,350,141\n",
       "Trainable params: 7,348,477\n",
       "Non-trainable params: 1,664\n",
       "Total mult-adds (Units.MEGABYTES): 367.37\n",
       "===============================================================================================\n",
       "Input size (MB): 165.57\n",
       "Forward/backward pass size (MB): 9403.58\n",
       "Params size (MB): 29.40\n",
       "Estimated Total Size (MB): 9598.55\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model_summary=summary(model,input_size=(50,995,832))\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 49.56 MiB is free. Including non-PyTorch memory, this process has 3.14 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 121.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# data2 = torch.from_numpy(data2).float()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(data2)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(values)\n\u001b[1;32m      4\u001b[0m preds\u001b[38;5;241m=\u001b[39mpreds[\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m      5\u001b[0m preds\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<@beartype(iTransformer.iTransformer.iTransformer.forward) at 0x7a26b1b3d8a0>:67\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(__beartype_object_91188752, __beartype_get_violation, __beartype_conf, __beartype_object_134306612467136, __beartype_getrandbits, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/iTransformer/iTransformer.py:186\u001b[0m, in \u001b[0;36miTransformer.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# attention and feedforward layers\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attn, attn_post_norm, ff, ff_post_norm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 186\u001b[0m     x \u001b[38;5;241m=\u001b[39m attn(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    187\u001b[0m     x \u001b[38;5;241m=\u001b[39m attn_post_norm(x)\n\u001b[1;32m    188\u001b[0m     x \u001b[38;5;241m=\u001b[39m ff(x) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/iTransformer/iTransformer.py:66\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     64\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_qkv(x)\n\u001b[0;32m---> 66\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattend(q, k, v)\n\u001b[1;32m     68\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_v_gates(x)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_out(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/iTransformer/attend.py:123\u001b[0m, in \u001b[0;36mAttend.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    120\u001b[0m scale \u001b[38;5;241m=\u001b[39m default(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflash:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflash_attn(q, k, v)\n\u001b[1;32m    125\u001b[0m sim \u001b[38;5;241m=\u001b[39m einsum(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h i d, b h j d -> b h i j\u001b[39m\u001b[38;5;124m'\u001b[39m, q, k) \u001b[38;5;241m*\u001b[39m scale\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausal:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/iTransformer/attend.py:98\u001b[0m, in \u001b[0;36mAttend.flash_attn\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msdp_kernel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_asdict()):\n\u001b[0;32m---> 98\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m     99\u001b[0m         q, k, v,\n\u001b[1;32m    100\u001b[0m         is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausal,\n\u001b[1;32m    101\u001b[0m         dropout_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 49.56 MiB is free. Including non-PyTorch memory, this process has 3.14 GiB memory in use. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 121.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# data2 = torch.from_numpy(data2).float()\n",
    "values=torch.from_numpy(data2).float().to(\"cuda\")\n",
    "preds = model(values)\n",
    "preds=preds[5]\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data3),type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_predictions = (values == data3)\n",
    "# accuracy = np.mean(correct_predictions)\n",
    "\n",
    "# print(f'Element-wise comparison accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming values is a PyTorch tensor and data3 is a NumPy array\n",
    "# Detach the tensor and convert to numpy array\n",
    "predictions_np = values.detach().numpy()\n",
    "actuals_np = data3\n",
    "\n",
    "# Reshape arrays to 2D if necessary\n",
    "predictions_reshaped = predictions_np.reshape(-1, predictions_np.shape[-1])\n",
    "actuals_reshaped = actuals_np.reshape(-1, actuals_np.shape[-1])\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(actuals_reshaped, predictions_reshaped)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming values is a PyTorch tensor and data3 is a NumPy array\n",
    "# # Detach the tensor and convert to numpy array\n",
    "# predictions_np = values.detach().numpy()\n",
    "# actuals_np = data3\n",
    "\n",
    "# # Calculate the 5% tolerance\n",
    "# tolerance = 0.05 * actuals_np\n",
    "\n",
    "# # Check if predictions are within 5% of actuals\n",
    "# correct_predictions = np.abs(predictions_np - actuals_np) <= tolerance\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = np.mean(correct_predictions)\n",
    "\n",
    "# print(f'Accuracy within 5% error rate: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
